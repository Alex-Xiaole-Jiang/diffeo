{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu for inference\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "rng = np.random.default_rng(seed = 37)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch as t\n",
    "import torchvision as tv\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sys\n",
    "sys.path.append('utils')\n",
    "\n",
    "device = t.device(\"cuda\") if t.cuda.is_available() else t.device(\"cpu\")\n",
    "print(f'Using {device} for inference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV2 = tv.models.efficientnet_v2_s(weights = tv.models.EfficientNet_V2_S_Weights.DEFAULT).to(device)\n",
    "ENV2.eval();\n",
    "for param in ENV2.parameters():\n",
    "    param.requires_grad = False\n",
    "inference_transform = tv.models.EfficientNet_B0_Weights.IMAGENET1K_V1.transforms()\n",
    "inference_transforms = v2.Compose([\n",
    "    lambda x: x.convert('RGB'),\n",
    "    inference_transform,\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fav_pic_index = 28621\n",
    "data_root = 'imagenet'\n",
    "imagenet_val_inference = tv.datasets.ImageNet(data_root, split = 'val', transform = inference_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from distortion import dense_transform_amplitude, sparse_transform_amplitude, create_grid_sample, find_inv_grid, jacobian_det, get_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sparse_grid_sample(amplitudes, num_diffeo, x_cutoff = 10, y_cutoff = 10, num_terms = 3, x_resolution = 224, y_resolution = 224):\n",
    "  batch_grid_sample = []\n",
    "  # batch_inverse_grid_sample = []\n",
    "  for amp in amplitudes:\n",
    "    A_nm, B_nm = sparse_transform_amplitude(x_cutoff, y_cutoff, num_terms, amplitude = amp, loop = num_diffeo)\n",
    "    #A_nm = np.mean(A_nm, axis = 0)\n",
    "    #B_nm = np.mean(B_nm, axis = 0)\n",
    "    for A, B in zip(A_nm, B_nm):\n",
    "      grid_sample = create_grid_sample(x_resolution, y_resolution, A, B)\n",
    "    #inverse_grid_sample, _ = find_inv_grid(grid_sample)\n",
    "      batch_grid_sample.append(grid_sample)\n",
    "    #batch_inverse_grid_sample.append(inverse_grid_sample)\n",
    "  grid_sample = t.cat(batch_grid_sample)\n",
    "  return grid_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = {}\n",
    "def getActivation(name):\n",
    "    # the hook signature\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "handles = []\n",
    "\n",
    "def retrieve_layer_activation(model, input, layer_index):\n",
    "  if len(input) == 3: input = input[None, :, :, :]\n",
    "\n",
    "  layers = list(model.children())\n",
    "  layers_flat = flatten(layers)\n",
    "\n",
    "  for index in layer_index:\n",
    "    handles.append(layers_flat[index - 1].register_forward_hook(getActivation(str(index))))\n",
    "\n",
    "  with t.no_grad(): model(input)\n",
    "  for handle in handles: handle.remove()\n",
    "\n",
    "  return\n",
    "\n",
    "def flatten(array):\n",
    "    result = []\n",
    "    for element in array:\n",
    "        if hasattr(element, \"__iter__\"):\n",
    "            result.extend(flatten(element))\n",
    "        else:\n",
    "            result.append(element)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_steps = 50\n",
    "diffeo_num = 50\n",
    "diffeo_amp = t.linspace(0, 1.5, amp_steps)\n",
    "grid_sample = get_sparse_grid_sample(diffeo_amp, diffeo_num, x_cutoff= 10, y_cutoff = 10, num_terms = 3, x_resolution =224, y_resolution= 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2500, 224, 224, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = '/content/drive/Shareddrives/Diffeomorphisms_&_CNNs/Alex/data/'\n",
    "#t.save(grid_sample, path + '20-50-10-10-3-224-224_grid_sample.pt')\n",
    "last_layer_activation = []\n",
    "imagenet_val_loader = iter(t.utils.data.DataLoader(imagenet_val_inference, batch_size = 1, shuffle=False))\n",
    "for i in tqdm(range(100)):\n",
    "  #file_prefix = f'20-50-10-10-3-224-224_image-{i:04d}_activation'\n",
    "  val_image, _ = next(imagenet_val_loader)\n",
    "  distorted_list = t.nn.functional.grid_sample(val_image.repeat(amp_steps * diffeo_num,1,1,1), grid_sample, mode = 'bilinear').to(device)\n",
    "  retrieve_layer_activation(ENV2, distorted_list, [49])\n",
    "  last_layer_activation.append(activation['49'])\n",
    "  activation = {}\n",
    "  handle = []\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
