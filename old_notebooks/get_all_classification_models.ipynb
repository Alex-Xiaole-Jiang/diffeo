{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available classification models:\n",
      "- alexnet\n",
      "- convnext_base\n",
      "- convnext_large\n",
      "- convnext_small\n",
      "- convnext_tiny\n",
      "- densenet121\n",
      "- densenet161\n",
      "- densenet169\n",
      "- densenet201\n",
      "- efficientnet_b0\n",
      "- efficientnet_b1\n",
      "- efficientnet_b2\n",
      "- efficientnet_b3\n",
      "- efficientnet_b4\n",
      "- efficientnet_b5\n",
      "- efficientnet_b6\n",
      "- efficientnet_b7\n",
      "- efficientnet_v2_l\n",
      "- efficientnet_v2_m\n",
      "- efficientnet_v2_s\n",
      "- googlenet\n",
      "- inception_v3\n",
      "- maxvit_t\n",
      "- mnasnet0_5\n",
      "- mnasnet0_75\n",
      "- mnasnet1_0\n",
      "- mnasnet1_3\n",
      "- mobilenet_v2\n",
      "- mobilenet_v3_large\n",
      "- mobilenet_v3_small\n",
      "- regnet_x_16gf\n",
      "- regnet_x_1_6gf\n",
      "- regnet_x_32gf\n",
      "- regnet_x_3_2gf\n",
      "- regnet_x_400mf\n",
      "- regnet_x_800mf\n",
      "- regnet_x_8gf\n",
      "- regnet_y_128gf\n",
      "- regnet_y_16gf\n",
      "- regnet_y_1_6gf\n",
      "- regnet_y_32gf\n",
      "- regnet_y_3_2gf\n",
      "- regnet_y_400mf\n",
      "- regnet_y_800mf\n",
      "- regnet_y_8gf\n",
      "- resnet101\n",
      "- resnet152\n",
      "- resnet18\n",
      "- resnet34\n",
      "- resnet50\n",
      "- resnext101_32x8d\n",
      "- resnext101_64x4d\n",
      "- resnext50_32x4d\n",
      "- shufflenet_v2_x0_5\n",
      "- shufflenet_v2_x1_0\n",
      "- shufflenet_v2_x1_5\n",
      "- shufflenet_v2_x2_0\n",
      "- squeezenet1_0\n",
      "- squeezenet1_1\n",
      "- swin_b\n",
      "- swin_s\n",
      "- swin_t\n",
      "- swin_v2_b\n",
      "- swin_v2_s\n",
      "- swin_v2_t\n",
      "- vgg11\n",
      "- vgg11_bn\n",
      "- vgg13\n",
      "- vgg13_bn\n",
      "- vgg16\n",
      "- vgg16_bn\n",
      "- vgg19\n",
      "- vgg19_bn\n",
      "- vit_b_16\n",
      "- vit_b_32\n",
      "- vit_h_14\n",
      "- vit_l_16\n",
      "- vit_l_32\n",
      "- wide_resnet101_2\n",
      "- wide_resnet50_2\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "import inspect\n",
    "\n",
    "def get_classification_models():\n",
    "    \"\"\"Get all image classification models from torchvision.models\"\"\"\n",
    "    \n",
    "    # Get all callable objects from torchvision.models\n",
    "    all_models = {name: obj for name, obj in inspect.getmembers(models, callable)}\n",
    "    \n",
    "    # Filter out non-classification models (detection, segmentation, etc.)\n",
    "    # Classification models take 'weights' parameter in newer PyTorch versions\n",
    "    classification_models = {\n",
    "        name: obj for name, obj in all_models.items() \n",
    "        if 'weights' in inspect.signature(obj).parameters\n",
    "    }\n",
    "    \n",
    "    return classification_models\n",
    "\n",
    "def get_model_transforms(model_name: str):\n",
    "    \"\"\"Get the inference transforms for a given model name\"\"\"\n",
    "    # Get the weights enum class (e.g., ResNet50_Weights)\n",
    "    weights_enum = getattr(models, f\"{model_name}_Weights\")\n",
    "    \n",
    "    # Get the default weights\n",
    "    weights = weights_enum.DEFAULT\n",
    "    \n",
    "    # Get the transforms\n",
    "    transforms = weights.transforms()\n",
    "    \n",
    "    return transforms\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    models_dict = get_classification_models()\n",
    "    print(\"\\nAvailable classification models:\")\n",
    "    for name in sorted(models_dict.keys()):\n",
    "        print(f\"- {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "getattr(models, f\"{model_name}_Weights\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Transforms for ResNet50:\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ImageClassification' object has no attribute 'transforms'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTransforms for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m transforms \u001b[38;5;241m=\u001b[39m get_model_transforms(name)\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m:\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/ext3/miniforge3/lib/python3.12/site-packages/torch/nn/modules/module.py:1931\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1929\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1930\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1931\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1932\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1933\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ImageClassification' object has no attribute 'transforms'"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example usage with a few models\n",
    "    model_names = [\"ResNet50\", \"VGG16\", \"DenseNet121\"]\n",
    "    \n",
    "    for name in model_names:\n",
    "        print(f\"\\nTransforms for {name}:\")\n",
    "        transforms = get_model_transforms(name)\n",
    "        for t in transforms.transforms:\n",
    "            print(f\"- {t.__class__.__name__}: {t}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diffeo_singularity",
   "language": "python",
   "name": "diffeo_singularity"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
